#!/usr/bin/env python3
"""
Version 6 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

è¤‡æ•°ã®ç•°ãªã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€
äºˆæ¸¬ã‚’çµ±åˆã™ã‚‹ã“ã¨ã§ãƒ­ãƒã‚¹ãƒˆæ€§ã¨ç²¾åº¦ã‚’å‘ä¸Š
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelEncoder
import json


def precision_at_k(y_true, y_pred, k=100):
    """Top Käºˆæ¸¬ã®Precision"""
    if len(y_true) < k:
        k = len(y_true)
    top_k_idx = np.argsort(y_pred)[-k:]
    return y_true.iloc[top_k_idx].mean() if hasattr(y_true, 'iloc') else y_true[top_k_idx].mean()


def train_single_model(X, y, params, n_folds=5, model_name="model"):
    """
    å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨OOFäºˆæ¸¬ã®ç”Ÿæˆ

    Returns:
        oof_predictions, fold_models, fold_metrics
    """
    print(f"\n  === {model_name} è¨“ç·´ä¸­ ===")

    tscv = TimeSeriesSplit(n_splits=n_folds)

    oof_predictions = np.zeros(len(y))
    fold_models = []
    fold_metrics = []

    for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X)):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

        # è¨“ç·´
        model = lgb.train(
            params,
            train_data,
            num_boost_round=300,
            valid_sets=[val_data],
            callbacks=[lgb.early_stopping(stopping_rounds=30), lgb.log_evaluation(0)]
        )

        # äºˆæ¸¬
        val_pred = model.predict(X_val, num_iteration=model.best_iteration)
        oof_predictions[val_idx] = val_pred

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        roc_auc = roc_auc_score(y_val, val_pred)
        prec_k = precision_at_k(y_val, val_pred, k=100)

        fold_metrics.append({
            'fold': fold_idx + 1,
            'roc_auc': roc_auc,
            'precision_at_top_k': prec_k,
            'best_iteration': model.best_iteration
        })

        fold_models.append(model)

        print(f"    Fold {fold_idx+1}: ROC-AUC={roc_auc:.4f}, P@100={prec_k:.2f}, Iter={model.best_iteration}")

    # OOFå…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    oof_roc_auc = roc_auc_score(y, oof_predictions)
    oof_prec_k = precision_at_k(y, oof_predictions, k=100)

    print(f"  â†’ {model_name} OOF: ROC-AUC={oof_roc_auc:.4f}, P@100={oof_prec_k:.4f}")

    return oof_predictions, fold_models, fold_metrics


def train_ensemble(X, y, n_folds=5):
    """
    ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ï¼šè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦äºˆæ¸¬ã‚’çµ±åˆ

    Returns:
        ensemble_oof, all_models_info
    """
    print("\n=== ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’é–‹å§‹ ===")

    # V5ã®ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€3ã¤ã®ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    base_params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'verbose': -1,
        'seed': 42,
    }

    # ãƒ¢ãƒ‡ãƒ«1: V5ãƒ™ã‚¹ãƒˆï¼ˆãƒãƒ©ãƒ³ã‚¹å‹ï¼‰
    params1 = base_params.copy()
    params1.update({
        'learning_rate': 0.03,
        'num_leaves': 31,
        'max_depth': 8,
        'min_child_samples': 30,
        'scale_pos_weight': 2.5,
    })

    # ãƒ¢ãƒ‡ãƒ«2: ã‚ˆã‚Šä¿å®ˆçš„ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
    params2 = base_params.copy()
    params2.update({
        'learning_rate': 0.02,
        'num_leaves': 31,
        'max_depth': 6,
        'min_child_samples': 50,
        'scale_pos_weight': 2.5,
        'feature_fraction': 0.8,  # ãƒ©ãƒ³ãƒ€ãƒ ç‰¹å¾´é‡é¸æŠ
        'bagging_fraction': 0.8,  # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        'bagging_freq': 5,
    })

    # ãƒ¢ãƒ‡ãƒ«3: ã‚ˆã‚Šè¤‡é›‘ï¼ˆé«˜è¡¨ç¾åŠ›ï¼‰
    params3 = base_params.copy()
    params3.update({
        'learning_rate': 0.04,
        'num_leaves': 63,
        'max_depth': 10,
        'min_child_samples': 20,
        'scale_pos_weight': 2.5,
        'lambda_l1': 0.1,  # L1æ­£å‰‡åŒ–
        'lambda_l2': 0.1,  # L2æ­£å‰‡åŒ–
    })

    # å„ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    models_info = []

    oof1, models1, metrics1 = train_single_model(X, y, params1, n_folds, "Model1_Balanced")
    models_info.append({'name': 'Model1_Balanced', 'params': params1, 'oof': oof1, 'models': models1, 'metrics': metrics1})

    oof2, models2, metrics2 = train_single_model(X, y, params2, n_folds, "Model2_Conservative")
    models_info.append({'name': 'Model2_Conservative', 'params': params2, 'oof': oof2, 'models': models2, 'metrics': metrics2})

    oof3, models3, metrics3 = train_single_model(X, y, params3, n_folds, "Model3_Complex")
    models_info.append({'name': 'Model3_Complex', 'params': params3, 'oof': oof3, 'models': models3, 'metrics': metrics3})

    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ï¼ˆå˜ç´”å¹³å‡ï¼‰
    print("\n  === ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã®çµ±åˆ ===")
    ensemble_oof = (oof1 + oof2 + oof3) / 3

    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    ensemble_roc_auc = roc_auc_score(y, ensemble_oof)
    ensemble_prec_k = precision_at_k(y, ensemble_oof, k=100)

    print(f"  â†’ Ensemble OOF: ROC-AUC={ensemble_roc_auc:.4f}, P@100={ensemble_prec_k:.4f}")

    # å„ãƒ¢ãƒ‡ãƒ«ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®æ¯”è¼ƒ
    print("\n  === ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ ===")
    for info in models_info:
        oof_roc = roc_auc_score(y, info['oof'])
        oof_prec = precision_at_k(y, info['oof'], k=100)
        print(f"    {info['name']}: ROC-AUC={oof_roc:.4f}, P@100={oof_prec:.4f}")

    print(f"    Ensemble:      ROC-AUC={ensemble_roc_auc:.4f}, P@100={ensemble_prec_k:.4f} âœ…")

    return ensemble_oof, models_info


def main():
    print("=== Version 6 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨“ç·´é–‹å§‹ ===\n")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    df = pd.read_csv('data/training_dataset_ultra_v6.csv')
    print(f"   {len(df):,}è¡Œ, {len(df.columns)}åˆ—")

    # ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®åˆ†é›¢
    target_col = 'target_high_payout'
    exclude_cols = ['category', 'grade', 'keirin_cd', 'race_date', 'track', target_col]

    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«çµ„ã¿åˆã‚ã›ã‚«ãƒ©ãƒ ã®å‡¦ç†
    categorical_comb_cols = [c for c in df.columns if '_x_' in c and df[c].dtype == 'object']

    # Label Encodingã§ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«çµ„ã¿åˆã‚ã›ã‚’æ•°å€¤åŒ–
    print("\n2. ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«çµ„ã¿åˆã‚ã›ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¸­...")
    for col in categorical_comb_cols:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))
        print(f"   {col}: {df[col].nunique()}ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤")

    # ç‰¹å¾´é‡é¸æŠ
    feature_cols = [c for c in df.columns if c not in exclude_cols]

    X = df[feature_cols]
    y = df[target_col]

    print(f"\n3. è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†")
    print(f"   ç‰¹å¾´é‡æ•°: {len(feature_cols)}")
    print(f"   Positive rate: {y.mean():.3f}")

    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨“ç·´
    print("\n4. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨“ç·´é–‹å§‹...")
    ensemble_oof, models_info = train_ensemble(X, y, n_folds=5)

    # çµæœä¿å­˜
    print("\n5. çµæœä¿å­˜ä¸­...")

    # OOFäºˆæ¸¬ä¿å­˜
    oof_df = df[['race_date', 'track', target_col]].copy()
    oof_df['ensemble_prediction'] = ensemble_oof

    # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚‚ä¿å­˜
    for info in models_info:
        oof_df[f'{info["name"]}_prediction'] = info['oof']

    oof_file = 'analysis/model_outputs/high_payout_model_v6_ensemble_oof.csv'
    oof_df.to_csv(oof_file, index=False)
    print(f"   OOFäºˆæ¸¬ä¿å­˜: {oof_file}")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    ensemble_roc_auc = roc_auc_score(y, ensemble_oof)
    ensemble_prec_k = precision_at_k(y, ensemble_oof, k=100)

    metadata = {
        'version': 'v6_ensemble',
        'n_features': len(feature_cols),
        'n_samples': len(df),
        'positive_rate': float(y.mean()),
        'ensemble_metrics': {
            'oof_roc_auc': float(ensemble_roc_auc),
            'oof_precision_at_top_k': float(ensemble_prec_k),
        },
        'models': []
    }

    for info in models_info:
        model_oof_roc = roc_auc_score(y, info['oof'])
        model_oof_prec = precision_at_k(y, info['oof'], k=100)

        metadata['models'].append({
            'name': info['name'],
            'params': info['params'],
            'oof_roc_auc': float(model_oof_roc),
            'oof_precision_at_top_k': float(model_oof_prec),
            'folds': info['metrics']
        })

    metadata_file = 'analysis/model_outputs/high_payout_model_v6_ensemble_metadata.json'
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"   ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {metadata_file}")

    # å„ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ï¼ˆFold 0ã®ãƒ¢ãƒ‡ãƒ«ï¼‰
    for info in models_info:
        model_file = f'analysis/model_outputs/high_payout_model_v6_{info["name"]}.txt'
        info['models'][0].save_model(model_file)
        print(f"   {info['name']} ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")

    # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆModel1ã‹ã‚‰å–å¾—ï¼‰
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'gain': models_info[0]['models'][0].feature_importance(importance_type='gain')
    }).sort_values('gain', ascending=False)

    fi_file = 'analysis/model_outputs/high_payout_model_v6_feature_importance.csv'
    feature_importance.to_csv(fi_file, index=False)
    print(f"   ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜: {fi_file}")

    print("\n=== Top 20 é‡è¦ç‰¹å¾´é‡ ===")
    for idx, row in feature_importance.head(20).iterrows():
        print(f"  {row['feature']}: {row['gain']:.1f}")

    print("\nâœ… Version 6 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨“ç·´å®Œäº†ï¼")

    # æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "="*60)
    print("ğŸ“Š V6 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« æœ€çµ‚çµæœ")
    print("="*60)
    print(f"ç‰¹å¾´é‡æ•°: {len(feature_cols)}")
    print(f"ãƒ¬ãƒ¼ã‚¹æ•°: {len(df):,}")
    print(f"\nã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ€§èƒ½:")
    print(f"  ROC-AUC (OOF):        {ensemble_roc_auc:.4f}")
    print(f"  Precision@Top100:     {ensemble_prec_k:.4f} ({ensemble_prec_k*100:.1f}%)")
    print("="*60)


if __name__ == '__main__':
    main()
