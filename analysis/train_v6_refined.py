#!/usr/bin/env python3
"""
Version 6 Refined: é‡è¦ç‰¹å¾´é‡ã®ã¿ã‚’ä½¿ç”¨ã—ãŸæœ€é©åŒ–è¨“ç·´

V6ã®æ–°ç‰¹å¾´é‡ã®ã†ã¡ã€é‡è¦åº¦ãŒé«˜ã„ã‚‚ã®ã ã‘ã‚’é¸æŠã—ã€
V5ã®ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è¨“ç·´ã™ã‚‹ã“ã¨ã§ã€éå­¦ç¿’ã‚’é˜²ãã¤ã¤ç²¾åº¦å‘ä¸Šã‚’ç›®æŒ‡ã™
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelEncoder
import json


def precision_at_k(y_true, y_pred, k=100):
    """Top Käºˆæ¸¬ã®Precision"""
    if len(y_true) < k:
        k = len(y_true)
    top_k_idx = np.argsort(y_pred)[-k:]
    return y_true.iloc[top_k_idx].mean() if hasattr(y_true, 'iloc') else y_true[top_k_idx].mean()


def select_top_features(df, feature_importance_file, top_n=80):
    """
    ç‰¹å¾´é‡é‡è¦åº¦ã«åŸºã¥ã„ã¦ä¸Šä½Nå€‹ã®ç‰¹å¾´é‡ã‚’é¸æŠ

    V5ã¨V6ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®ä¸¡æ–¹ã®é‡è¦åº¦ã‚’å‚ç…§
    """
    print(f"\n  ç‰¹å¾´é‡é¸æŠ: ä¸Šä½{top_n}å€‹ã‚’ä½¿ç”¨")

    # V6ã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’èª­ã¿è¾¼ã¿
    v6_fi = pd.read_csv(feature_importance_file)

    # V5ã®ç‰¹å¾´é‡é‡è¦åº¦ã‚‚èª­ã¿è¾¼ã¿ï¼ˆå‚è€ƒï¼‰
    try:
        v5_fi = pd.read_csv('analysis/model_outputs/high_payout_model_v5_feature_importance.csv')
        # ä¸¡æ–¹ã®é‡è¦åº¦ã®å¹³å‡ã‚’å–ã‚‹
        merged_fi = v6_fi.merge(v5_fi, on='feature', how='outer', suffixes=('_v6', '_v5'))
        merged_fi['gain_v6'] = merged_fi['gain_v6'].fillna(0)
        merged_fi['gain_v5'] = merged_fi['gain_v5'].fillna(0)
        merged_fi['gain_avg'] = (merged_fi['gain_v6'] + merged_fi['gain_v5']) / 2
        merged_fi = merged_fi.sort_values('gain_avg', ascending=False)

        top_features = merged_fi.head(top_n)['feature'].tolist()
        print(f"  â†’ V5ã¨V6ã®é‡è¦åº¦ã‚’çµ±åˆã—ã¦é¸æŠ")

    except:
        # V5ãŒãªã‘ã‚Œã°V6ã ã‘ä½¿ã†
        v6_fi = v6_fi.sort_values('gain', ascending=False)
        top_features = v6_fi.head(top_n)['feature'].tolist()
        print(f"  â†’ V6ã®é‡è¦åº¦ã®ã¿ã§é¸æŠ")

    # åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿
    available_features = [f for f in top_features if f in df.columns]

    print(f"  â†’ é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡: {len(available_features)}å€‹")

    return available_features


def main():
    print("=== Version 6 Refined è¨“ç·´é–‹å§‹ ===\n")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    df = pd.read_csv('data/training_dataset_ultra_v6.csv')
    print(f"   {len(df):,}è¡Œ, {len(df.columns)}åˆ—")

    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«çµ„ã¿åˆã‚ã›ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    print("\n2. ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«çµ„ã¿åˆã‚ã›ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¸­...")
    categorical_comb_cols = [c for c in df.columns if '_x_' in c and df[c].dtype == 'object']
    for col in categorical_comb_cols:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))

    # ç‰¹å¾´é‡é¸æŠ
    target_col = 'target_high_payout'
    exclude_cols = ['category', 'grade', 'keirin_cd', 'race_date', 'track', target_col]

    all_feature_cols = [c for c in df.columns if c not in exclude_cols]

    # V6ã®ç‰¹å¾´é‡é‡è¦åº¦ã«åŸºã¥ã„ã¦ä¸Šä½80å€‹ã‚’é¸æŠ
    print("\n3. ç‰¹å¾´é‡é¸æŠä¸­...")
    selected_features = select_top_features(
        df,
        'analysis/model_outputs/high_payout_model_v6_feature_importance.csv',
        top_n=80
    )

    X = df[selected_features]
    y = df[target_col]

    print(f"\n4. è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†")
    print(f"   ç‰¹å¾´é‡æ•°: {len(selected_features)}")
    print(f"   Positive rate: {y.mean():.3f}")

    # V5ã®ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
    print("\n5. V5ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è¨“ç·´é–‹å§‹...")
    best_params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'verbose': -1,
        'seed': 42,
        'learning_rate': 0.03,
        'num_leaves': 31,
        'max_depth': 8,
        'min_child_samples': 30,
        'scale_pos_weight': 2.5,
    }

    tscv = TimeSeriesSplit(n_splits=5)

    oof_predictions = np.zeros(len(y))
    fold_models = []
    fold_metrics = []

    for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X)):
        print(f"\n  Fold {fold_idx+1}/5 è¨“ç·´ä¸­...")
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

        model = lgb.train(
            best_params,
            train_data,
            num_boost_round=300,
            valid_sets=[val_data],
            callbacks=[lgb.early_stopping(stopping_rounds=30), lgb.log_evaluation(50)]
        )

        # äºˆæ¸¬
        val_pred = model.predict(X_val, num_iteration=model.best_iteration)
        oof_predictions[val_idx] = val_pred

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        roc_auc = roc_auc_score(y_val, val_pred)
        prec_k = precision_at_k(y_val, val_pred, k=100)

        fold_metrics.append({
            'fold': fold_idx + 1,
            'roc_auc': roc_auc,
            'precision_at_top_k': prec_k,
            'best_iteration': model.best_iteration
        })

        fold_models.append(model)

        print(f"  â†’ ROC-AUC: {roc_auc:.4f}, Precision@Top100: {prec_k:.2f}, Best iteration: {model.best_iteration}")

    # OOFå…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    oof_roc_auc = roc_auc_score(y, oof_predictions)
    oof_prec_k = precision_at_k(y, oof_predictions, k=100)

    print(f"\n=== OOFå…¨ä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ ===")
    print(f"ROC-AUC: {oof_roc_auc:.4f}")
    print(f"Precision@Top100: {oof_prec_k:.4f} ({oof_prec_k*100:.1f}%)")

    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
    print("\n6. æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ä¸­...")
    train_data = lgb.Dataset(X, label=y)
    final_model = lgb.train(
        best_params,
        train_data,
        num_boost_round=int(np.mean([m['best_iteration'] for m in fold_metrics])),
        callbacks=[lgb.log_evaluation(50)]
    )

    # ä¿å­˜
    print("\n7. çµæœä¿å­˜ä¸­...")
    model_file = 'analysis/model_outputs/high_payout_model_v6_refined.txt'
    final_model.save_model(model_file)
    print(f"   ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    metadata = {
        'version': 'v6_refined',
        'n_features': len(selected_features),
        'n_samples': len(df),
        'positive_rate': float(y.mean()),
        'selected_features': selected_features,
        'params': best_params,
        'metrics': {
            'oof_roc_auc': float(oof_roc_auc),
            'oof_precision_at_top_k': float(oof_prec_k),
            'folds': fold_metrics
        }
    }

    metadata_file = 'analysis/model_outputs/high_payout_model_v6_refined_metadata.json'
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"   ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {metadata_file}")

    # OOFäºˆæ¸¬
    oof_df = df[['race_date', 'track', target_col]].copy()
    oof_df['prediction'] = oof_predictions
    oof_file = 'analysis/model_outputs/high_payout_model_v6_refined_oof.csv'
    oof_df.to_csv(oof_file, index=False)
    print(f"   OOFäºˆæ¸¬ä¿å­˜: {oof_file}")

    # ç‰¹å¾´é‡é‡è¦åº¦
    feature_importance = pd.DataFrame({
        'feature': selected_features,
        'gain': final_model.feature_importance(importance_type='gain')
    }).sort_values('gain', ascending=False)

    fi_file = 'analysis/model_outputs/high_payout_model_v6_refined_feature_importance.csv'
    feature_importance.to_csv(fi_file, index=False)
    print(f"   ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜: {fi_file}")

    print("\n=== Top 20 é‡è¦ç‰¹å¾´é‡ ===")
    for idx, row in feature_importance.head(20).iterrows():
        print(f"  {row['feature']}: {row['gain']:.1f}")

    print("\nâœ… Version 6 Refined è¨“ç·´å®Œäº†ï¼")

    # æ¯”è¼ƒè¡¨ç¤º
    print("\n" + "="*70)
    print("ğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ")
    print("="*70)
    print(f"{'ãƒãƒ¼ã‚¸ãƒ§ãƒ³':<20} {'ç‰¹å¾´é‡æ•°':<12} {'Precision@Top100':<20}")
    print("-"*70)
    print(f"{'V5 (ãƒ™ã‚¹ãƒˆ)':<20} {84:<12} {67.0:<20.1f}%")
    print(f"{'V6 Ensemble':<20} {112:<12} {59.0:<20.1f}%")
    print(f"{'V6 Refined':<20} {len(selected_features):<12} {oof_prec_k*100:<20.1f}%")
    print("="*70)


if __name__ == '__main__':
    main()
