#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç«¶è¼ªé«˜é…å½“äºˆæ¸¬ï¼šã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰
äº‹å¾Œãƒ‡ãƒ¼ã‚¿ã‚’å®Œå…¨ã«é™¤å¤–ã—ã€äº‹å‰ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ç‰¹å¾´é‡ã‚’æ§‹ç¯‰
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys

# Regional line mapping (ç«¶è¼ªã®ãƒ©ã‚¤ãƒ³: åœ°åŸŸåˆ¥å…±åŒä½œæˆ¦)
REGIONAL_LINES = {
    # åŒ—æ—¥æœ¬ãƒ©ã‚¤ãƒ³
    "åŒ—æµ·é“": "åŒ—æ—¥æœ¬", "é’æ£®": "åŒ—æ—¥æœ¬", "å²©æ‰‹": "åŒ—æ—¥æœ¬", "å®®åŸ": "åŒ—æ—¥æœ¬",
    "ç§‹ç”°": "åŒ—æ—¥æœ¬", "å±±å½¢": "åŒ—æ—¥æœ¬", "ç¦å³¶": "åŒ—æ—¥æœ¬",
    # é–¢æ±ãƒ©ã‚¤ãƒ³
    "èŒ¨åŸ": "é–¢æ±", "æ ƒæœ¨": "é–¢æ±", "ç¾¤é¦¬": "é–¢æ±", "åŸ¼ç‰": "é–¢æ±",
    "åƒè‘‰": "é–¢æ±", "æ±äº¬": "é–¢æ±", "ç¥å¥ˆå·": "é–¢æ±", "å±±æ¢¨": "é–¢æ±",
    # åŒ—é™¸ãƒ©ã‚¤ãƒ³
    "æ–°æ½Ÿ": "åŒ—é™¸", "å¯Œå±±": "åŒ—é™¸", "çŸ³å·": "åŒ—é™¸", "ç¦äº•": "åŒ—é™¸",
    # ä¸­éƒ¨ãƒ©ã‚¤ãƒ³
    "é•·é‡": "ä¸­éƒ¨", "å²é˜œ": "ä¸­éƒ¨", "é™å²¡": "ä¸­éƒ¨", "æ„›çŸ¥": "ä¸­éƒ¨",
    # è¿‘ç•¿ãƒ©ã‚¤ãƒ³
    "ä¸‰é‡": "è¿‘ç•¿", "æ»‹è³€": "è¿‘ç•¿", "äº¬éƒ½": "è¿‘ç•¿", "å¤§é˜ª": "è¿‘ç•¿",
    "å…µåº«": "è¿‘ç•¿", "å¥ˆè‰¯": "è¿‘ç•¿", "å’Œæ­Œå±±": "è¿‘ç•¿",
    # ä¸­å›½ãƒ©ã‚¤ãƒ³
    "é³¥å–": "ä¸­å›½", "å³¶æ ¹": "ä¸­å›½", "å²¡å±±": "ä¸­å›½", "åºƒå³¶": "ä¸­å›½", "å±±å£": "ä¸­å›½",
    # å››å›½ãƒ©ã‚¤ãƒ³
    "å¾³å³¶": "å››å›½", "é¦™å·": "å››å›½", "æ„›åª›": "å››å›½", "é«˜çŸ¥": "å››å›½",
    # ä¹å·ãƒ©ã‚¤ãƒ³
    "ç¦å²¡": "ä¹å·", "ä½è³€": "ä¹å·", "é•·å´": "ä¹å·", "ç†Šæœ¬": "ä¹å·",
    "å¤§åˆ†": "ä¹å·", "å®®å´": "ä¹å·", "é¹¿å…å³¶": "ä¹å·", "æ²–ç¸„": "ä¹å·",
}

def get_regional_line(prefecture):
    """åºœçœŒã‹ã‚‰ãƒ©ã‚¤ãƒ³ï¼ˆåœ°åŸŸï¼‰ã‚’å–å¾—"""
    pref = str(prefecture).strip().replace('ã€€', '')
    return REGIONAL_LINES.get(pref, "ãã®ä»–")


def normalize_style(kyakusitu):
    """è„šè³ªã‚’æ­£è¦åŒ–"""
    s = str(kyakusitu).strip()
    if 'é€ƒ' in s or 'å…ˆ' in s or 'æ²' in s:
        return 'nige'
    elif 'è¿½' in s or 'å·®' in s or 'ãƒãƒ¼ã‚¯' in s:
        return 'tsui'
    elif 'ä¸¡' in s or 'è‡ªåœ¨' in s:
        return 'ryo'
    return 'unknown'


def normalize_grade(kyuhan):
    """ç´šç­ã‚’æ­£è¦åŒ–"""
    g = str(kyuhan).strip().upper()
    if g in ['SS', 'S1', 'S2', 'A1', 'A2', 'A3', 'L1']:
        return g
    return 'unknown'


def calculate_entropy(counts):
    """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆå¤šæ§˜æ€§æŒ‡æ¨™ï¼‰"""
    total = sum(counts.values())
    if total == 0:
        return 0.0
    probs = [c / total for c in counts.values() if c > 0]
    return -sum(p * np.log2(p) for p in probs)


def aggregate_race_features(race_df):
    """
    ãƒ¬ãƒ¼ã‚¹å˜ä½ã§é¸æ‰‹æƒ…å ±ã‚’é›†ç´„ã—ã€äº‹å‰ãƒ‡ãƒ¼ã‚¿ã®ã¿ã®ç‰¹å¾´é‡ã‚’æ§‹ç¯‰
    """
    # Convert race_date to integer YYYYMMDD format
    race_date_raw = race_df['race_date'].iloc[0]
    if isinstance(race_date_raw, str):
        # Convert '2024-01-01' to 20240101
        race_date_int = int(race_date_raw.replace('-', ''))
    else:
        race_date_int = int(race_date_raw)

    # Basic race info
    track = str(race_df['track'].iloc[0])
    keirin_cd = str(race_df['keirin_cd'].iloc[0]) if 'keirin_cd' in race_df.columns else track
    category = str(race_df['category'].iloc[0]) if 'category' in race_df.columns else ''

    race_info = {
        'race_date': race_date_int,
        'track': track,
        'keirin_cd': keirin_cd,
        'race_no': int(race_df['race_no'].iloc[0]),
        'grade': str(race_df['grade'].iloc[0]),
        'category': category,
    }

    # Target variable
    if 'high_payout_flag' in race_df.columns:
        race_info['target_high_payout'] = int(race_df['high_payout_flag'].iloc[0])
    elif 'trifecta_payout_value' in race_df.columns:
        payout = float(race_df['trifecta_payout_value'].iloc[0])
        race_info['target_high_payout'] = int(payout >= 10000)
    else:
        race_info['target_high_payout'] = 0

    # Entry count
    entry_count = len(race_df)
    race_info['entry_count'] = entry_count

    # === ç«¶èµ°å¾—ç‚¹ã®çµ±è¨ˆ ===
    scores = pd.to_numeric(race_df['heikinTokuten'], errors='coerce').dropna()
    if len(scores) > 0:
        race_info['score_mean'] = float(scores.mean())
        race_info['score_std'] = float(scores.std()) if len(scores) > 1 else 0.0
        race_info['score_min'] = float(scores.min())
        race_info['score_max'] = float(scores.max())
        race_info['score_range'] = race_info['score_max'] - race_info['score_min']
        race_info['score_median'] = float(scores.median())
        race_info['score_q25'] = float(scores.quantile(0.25))
        race_info['score_q75'] = float(scores.quantile(0.75))
        race_info['score_iqr'] = race_info['score_q75'] - race_info['score_q25']
        race_info['score_cv'] = race_info['score_std'] / (race_info['score_mean'] + 1e-6)

        # Top3 vs Bottom3
        top3 = scores.nlargest(min(3, len(scores)))
        bottom3 = scores.nsmallest(min(3, len(scores)))
        race_info['score_top3_mean'] = float(top3.mean())
        race_info['score_bottom3_mean'] = float(bottom3.mean())
        race_info['score_top_bottom_gap'] = race_info['score_top3_mean'] - race_info['score_bottom3_mean']

        # æ¨å®šäººæ°—åº¦ï¼ˆå¾—ç‚¹ãƒ™ãƒ¼ã‚¹ï¼‰
        sorted_scores = scores.sort_values(ascending=False)
        rank1 = float(sorted_scores.iloc[0]) if len(sorted_scores) > 0 else race_info['score_mean']
        rank2 = float(sorted_scores.iloc[1]) if len(sorted_scores) > 1 else rank1
        rank3 = float(sorted_scores.iloc[2]) if len(sorted_scores) > 2 else rank2

        race_info['estimated_top3_score_sum'] = rank1 + rank2 + rank3
        race_info['estimated_favorite_dominance'] = rank1 / (race_info['score_mean'] + 1e-6)
        race_info['estimated_favorite_gap'] = rank1 - rank2

        if len(scores) > 3:
            others_mean = float(scores.iloc[3:].mean())
            race_info['estimated_top3_vs_others'] = race_info['score_top3_mean'] - others_mean
        else:
            race_info['estimated_top3_vs_others'] = 0.0
    else:
        # Default values if no scores
        for key in ['score_mean', 'score_std', 'score_min', 'score_max', 'score_range',
                    'score_median', 'score_q25', 'score_q75', 'score_iqr', 'score_cv',
                    'score_top3_mean', 'score_bottom3_mean', 'score_top_bottom_gap',
                    'estimated_top3_score_sum', 'estimated_favorite_dominance',
                    'estimated_favorite_gap', 'estimated_top3_vs_others']:
            race_info[key] = 0.0

    # === è„šè³ªã®åˆ†æ ===
    styles = race_df['kyakusitu'].apply(normalize_style)
    style_counts = styles.value_counts().to_dict()

    for style in ['nige', 'tsui', 'ryo', 'unknown']:
        count = style_counts.get(style, 0)
        race_info[f'style_{style}_count'] = count
        race_info[f'style_{style}_ratio'] = count / entry_count if entry_count > 0 else 0.0

    # è„šè³ªã®å¤šæ§˜æ€§
    style_diversity = len([c for c in style_counts.values() if c > 0])
    race_info['style_diversity'] = style_diversity
    race_info['style_entropy'] = calculate_entropy(style_counts)

    ratios = [r for r in [race_info[f'style_{s}_ratio'] for s in ['nige', 'tsui', 'ryo']] if r > 0]
    race_info['style_max_ratio'] = max(ratios) if ratios else 0.0
    race_info['style_min_ratio'] = min(ratios) if ratios else 0.0

    # === ç´šç­ã®åˆ†æ ===
    grades = race_df['kyuhan'].apply(normalize_grade)
    grade_counts = grades.value_counts().to_dict()

    for grade in ['SS', 'S1', 'S2', 'A1', 'A2', 'A3', 'L1']:
        count = grade_counts.get(grade, 0)
        race_info[f'grade_{grade}_count'] = count
        race_info[f'grade_{grade}_ratio'] = count / entry_count if entry_count > 0 else 0.0

    race_info['grade_entropy'] = calculate_entropy(grade_counts)
    race_info['grade_has_mixed'] = int(len(grade_counts) > 1)

    # === ãƒ©ã‚¤ãƒ³ã®åˆ†æï¼ˆåœ°åŸŸåˆ¥ï¼‰ ===
    race_df['line'] = race_df['entry_prefecture'].apply(get_regional_line)
    line_counts = race_df['line'].value_counts().to_dict()

    race_info['line_count'] = len(line_counts)
    race_info['line_entropy'] = calculate_entropy(line_counts)

    if line_counts:
        dominant_line_count = max(line_counts.values())
        race_info['dominant_line_ratio'] = dominant_line_count / entry_count

        # ãƒ©ã‚¤ãƒ³åˆ¥ã®å¹³å‡å¾—ç‚¹å·®
        line_scores = race_df.groupby('line')['heikinTokuten'].apply(
            lambda x: pd.to_numeric(x, errors='coerce').mean()
        )
        race_info['line_balance_std'] = float(line_scores.std()) if len(line_scores) > 1 else 0.0
        race_info['line_score_gap'] = float(line_scores.max() - line_scores.min()) if len(line_scores) > 0 else 0.0
    else:
        race_info['dominant_line_ratio'] = 0.0
        race_info['line_balance_std'] = 0.0
        race_info['line_score_gap'] = 0.0

    # === åºœçœŒã®å¤šæ§˜æ€§ ===
    race_info['prefecture_unique_count'] = race_df['entry_prefecture'].nunique()

    # === è„šè³ªã‚«ã‚¦ãƒ³ãƒˆï¼ˆBé–¢é€£ï¼‰ã®åˆ†æ ===
    # é€ƒã’ã€æ²ã‚Šã€å·®ã—ã€ãƒãƒ¼ã‚¯ã€ãƒãƒƒã‚¯ã®å‹åˆ©å›æ•°ã¯é¸æ‰‹ã®æˆ¦ç¸¾ï¼ˆäº‹å‰ãƒ‡ãƒ¼ã‚¿ï¼‰
    b_columns = ['nigeCnt', 'makuriCnt', 'sasiCnt', 'markCnt', 'backCnt']

    for b_col in b_columns:
        if b_col in race_df.columns:
            b_values = pd.to_numeric(race_df[b_col], errors='coerce').fillna(0)

            race_info[f'{b_col}_mean'] = float(b_values.mean())
            race_info[f'{b_col}_std'] = float(b_values.std()) if len(b_values) > 1 else 0.0
            race_info[f'{b_col}_max'] = float(b_values.max())
            race_info[f'{b_col}_sum'] = float(b_values.sum())

            # CV (coefficient of variation)
            if race_info[f'{b_col}_mean'] > 0:
                race_info[f'{b_col}_cv'] = race_info[f'{b_col}_std'] / race_info[f'{b_col}_mean']
            else:
                race_info[f'{b_col}_cv'] = 0.0
        else:
            # Default values if column doesn't exist
            for suffix in ['mean', 'std', 'max', 'sum', 'cv']:
                race_info[f'{b_col}_{suffix}'] = 0.0

    # åˆè¨ˆçµŒé¨“å€¤ï¼ˆå…¨è„šè³ªã®åˆè¨ˆï¼‰
    total_b_experience = sum([race_info[f'{b}_sum'] for b in b_columns])
    race_info['total_b_experience'] = total_b_experience

    # çµŒé¨“å€¤ã®å¤šæ§˜æ€§ï¼ˆã©ã®è„šè³ªãŒå¼·ã„ã‹ï¼‰
    if total_b_experience > 0:
        b_distribution = {b: race_info[f'{b}_sum'] for b in b_columns}
        race_info['b_experience_entropy'] = calculate_entropy(b_distribution)
    else:
        race_info['b_experience_entropy'] = 0.0

    # === ãƒ¬ãƒ¼ã‚¹ç•ªå·ï¼ˆæ™‚é–“å¸¯ï¼‰ ===
    race_info['race_no_int'] = int(race_df['race_no'].iloc[0])

    # === æ—¥ä»˜é–¢é€£ ===
    race_date_str = str(int(race_info['race_date'])).zfill(8)
    try:
        dt = pd.to_datetime(race_date_str, format='%Y%m%d')
        race_info['year'] = dt.year
        race_info['month'] = dt.month
        race_info['day'] = dt.day
        race_info['day_of_week'] = dt.dayofweek
        race_info['is_weekend'] = int(dt.dayofweek >= 5)
    except:
        race_info['year'] = 0
        race_info['month'] = 0
        race_info['day'] = 0
        race_info['day_of_week'] = 0
        race_info['is_weekend'] = 0

    return race_info


def main():
    print("=" * 80)
    print("ç«¶è¼ªé«˜é…å½“äºˆæ¸¬ï¼šã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰")
    print("=" * 80)

    # Load raw data
    input_file = Path('data/keirin_training_dataset_20240101_20240331.csv')
    output_file = Path('data/clean_training_dataset.csv')

    if not input_file.exists():
        print(f"âŒ Error: {input_file} not found")
        sys.exit(1)

    print(f"\nğŸ“‚ Loading data from: {input_file}")
    df = pd.read_csv(input_file)
    print(f"âœ“ Loaded {len(df):,} rows, {len(df.columns)} columns")

    # Identify unique races
    print(f"\nğŸ” Identifying unique races...")
    race_groups = df.groupby(['race_date', 'track', 'race_no'])
    print(f"âœ“ Found {len(race_groups):,} unique races")

    # Aggregate features for each race
    print(f"\nâš™ï¸  Aggregating race-level features (äº‹å‰ãƒ‡ãƒ¼ã‚¿ã®ã¿)...")
    race_features = []

    for idx, (race_id, race_df) in enumerate(race_groups, 1):
        if idx % 500 == 0:
            print(f"  Processing race {idx:,} / {len(race_groups):,}...")

        try:
            features = aggregate_race_features(race_df)
            race_features.append(features)
        except Exception as e:
            print(f"âš ï¸  Warning: Failed to process race {race_id}: {e}")
            continue

    # Create final dataset
    print(f"\nğŸ“Š Creating final dataset...")
    clean_df = pd.DataFrame(race_features)

    # Sort by date
    clean_df = clean_df.sort_values(['race_date', 'keirin_cd', 'race_no_int']).reset_index(drop=True)

    # Save
    output_file.parent.mkdir(parents=True, exist_ok=True)
    clean_df.to_csv(output_file, index=False)

    print(f"\nâœ… Clean dataset created!")
    print(f"   Output: {output_file}")
    print(f"   Races: {len(clean_df):,}")
    print(f"   Features: {len(clean_df.columns)}")
    print(f"   High payout rate: {clean_df['target_high_payout'].mean():.1%}")

    # Feature summary
    print(f"\nğŸ“ˆ Feature Categories:")
    feature_groups = {
        'å¾—ç‚¹çµ±è¨ˆ': [c for c in clean_df.columns if 'score' in c],
        'è„šè³ªåˆ†æ': [c for c in clean_df.columns if 'style' in c],
        'ç´šç­åˆ†æ': [c for c in clean_df.columns if 'grade' in c and 'flag' not in c],
        'ãƒ©ã‚¤ãƒ³åˆ†æ': [c for c in clean_df.columns if 'line' in c],
        'æ¨å®šäººæ°—': [c for c in clean_df.columns if 'estimated' in c],
        'åŸºæœ¬æƒ…å ±': [c for c in clean_df.columns if c in ['entry_count', 'race_no_int', 'year', 'month', 'day_of_week', 'is_weekend', 'prefecture_unique_count']],
    }

    for group_name, features in feature_groups.items():
        print(f"   {group_name}: {len(features)} features")

    print("\n" + "=" * 80)
    print("âœ“ Complete! äº‹å¾Œãƒ‡ãƒ¼ã‚¿ã¯å®Œå…¨ã«é™¤å¤–ã•ã‚Œã¦ã„ã¾ã™ã€‚")
    print("=" * 80)


if __name__ == "__main__":
    main()
